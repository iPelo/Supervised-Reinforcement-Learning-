{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b7f3a738-486b-45d6-8f18-0e4cda691378",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Machine Learning in a Nutshell: Binary Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de8fb85a-8672-4242-8f02-2f1c99ddbe5d",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## A (given) Binary Classifier\n",
    "\n",
    "To begin, we assume the weights of a binary classifier are given as $w_1 = 1.0$ and $w_2 = 0.5$.\n",
    "In the second task we are going to learn the weights from data.\n",
    "\n",
    "We also assume, the feature extractor $\\phi(\\mathbf{x})$ is given as the identity function. For simplicity, we are working with the extracted feature vectors $[x_1, x_2]$ directly.\n",
    "\n",
    "### Our classifier\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "f_\\mathbf{w}(x) &= \\text{sign}(\\underbrace{[1, 0.5]}_{\\mathbf{w}} \\cdot \\underbrace{[x_1, x_2]}_{\\phi(x)})\\\\\n",
    "      &= \\text{sign}(\\underbrace{1}_{w_1} \\cdot x_1 + \\underbrace{0.5}_{w_2} \\cdot x_2)\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "### The sign-function maps from a score to a label\n",
    "$$\n",
    "\\begin{equation*}\n",
    "  \\text{sign}(z)=\\begin{cases}\n",
    "    +1, & \\text{if $z>0$}\\\\\n",
    "    -1, & \\text{if $z<0$}\\\\\n",
    "     0, & \\text{if $z = 0$}.\n",
    "  \\end{cases}\n",
    "\\end{equation*}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab7d5406",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Task 1: Making predictions with the classifier $f$\n",
    "\n",
    "In order to make predictions, we need to implement $f$ in Python. This requires us to implement the individual pieces as python methods:\n",
    "- A `getScore` method that computes the **score**: $w_1 \\cdot x_1 + w_2 \\cdot x_2$\n",
    "- A `getLabel` method that computes the **label** from a given **score**: $\\text{sign}(\\cdot)$\n",
    "- A `predict` method that computes the **label** from two parameters $x_1$ and $x_2$ by calling the other methods.\n",
    "\n",
    "Define a class and implement the `getScore`, `getLabel` and `predict` methods.\n",
    "The constructor should expect two parameters $w_0$ and $w_1$ and store the values in instance variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e033369a",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from typing import List\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "class BinaryClassifier:\n",
    "\n",
    "    def __init__(self, w1, w2):\n",
    "        pass\n",
    "\n",
    "    def getScore(self, x1, x2) -> float:\n",
    "        pass\n",
    "\n",
    "    def getLabel(self, z) -> int:\n",
    "        # returns the sign of the score\n",
    "        return 1 #TODO fix\n",
    "\n",
    "    def predict(self, x1, x2):\n",
    "        # predict the class of a data point (x1, x2)\n",
    "        pass\n",
    "\n",
    "    def getMargin(self, x1, x2, y) -> float:\n",
    "        pass\n",
    "\n",
    "    def getHingeLoss(self, x1, x2, y, minMargin=1.0) -> float:\n",
    "        # calculate the hinge loss for a single example (x1, x2, y)\n",
    "        pass\n",
    "\n",
    "    def verbosePrediction(self, x1, x2, y):\n",
    "        # TODO: just a \"debugging\" wrapper/ output function, has no intrinsic functionality, \n",
    "        # could be implemented as a verbose version of predict, if you prefer this\n",
    "        pass\n",
    "\n",
    "    def train(self, x1:float, x2:float, y:int, learningRate:float=0.1):\n",
    "        # Training with a weight update for a single point (what's the gradient?)\n",
    "        loss = self.getHingeLoss(x1, x2, y)\n",
    "        # pseudo code:\n",
    "        # get loss\n",
    "        # if loss > 0: update weights (self.weights) in the negative direction of the gradient\n",
    "\n",
    "    def train_batch(self, points: List, ys: List, learningRate:float=0.1):\n",
    "        # this is for one batch, the list contains one batch, this method would need to be called multiple times for a full dataset\n",
    "        # use Stochastic Gradient Descent Here. points contains a list of points.\n",
    "        # perform the gradient update for the average gradient computed using those points\n",
    "        # points is a list:\n",
    "        # ys is a length matching list containing the respective labels: e.g.: labels = [1, 1, 1, -1, -1, -1]\n",
    "        gradients = [] # keep track of the gradients for each batch (full iteration, remember to get the mean)\n",
    "        running_loss = [] # keep track of the losses, get the average for each epoch\n",
    "        for x, y in zip(points, ys):\n",
    "            pass\n",
    "            # get loss\n",
    "            loss = self.getHingeLoss(x1, x2, y)\n",
    "            # TODO: keep track of losses\n",
    "            if loss > 0:\n",
    "                # TODO: add gradient/ keep track\n",
    "                pass\n",
    "        print(f'batch loss{np.mean(running_loss)}')\n",
    "        # TODO; update the weights for this batch\n",
    "        pass\n",
    "        # TODO:return batch loss\n",
    "\n",
    "\n",
    "w1 = 1.0\n",
    "w2 = 0.5\n",
    "\n",
    "x1 = 2\n",
    "x2 = -2\n",
    "\n",
    "classifier = BinaryClassifier(w1, w2)\n",
    "# TODO\n",
    "classifier.predict(x1, x2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ad3f388-ba94-447e-abd4-7b517a246714",
   "metadata": {},
   "source": [
    "### "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ebe01b7",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Task 2: Multiple predictions and the target labels\n",
    "\n",
    "Below you are given a list of training examples.\n",
    "Each training example comes with a target label.\n",
    "\n",
    "The target label is the label our classifier should predict, ... but most likely it doesn't since we did not train it yet.\n",
    "\n",
    "Let's first make the classifier predict a label for each training example and print the predicted label next to the target label such we can see when things go wrong.\n",
    "\n",
    "#### Implement printing as a method of `BinaryClassifier`, named `verbosePrediction` that expects $x_1$, $x_2$ and $y$ as parameters and prints, for each training example, the predicted label and the target label and if it is correct."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2080091",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "data = [(0.5, 0.5), (2, 0), (-1, 1), (1, -1), (1, -2), (-1, -1)]\n",
    "labels = [1, 1, 1, -1, -1, -1]\n",
    "\n",
    "for x, y in zip(data, labels):\n",
    "    # TODO: commented out, because it won't work right from the start. you have to implement the methods first in BinaryClassifier\n",
    "    print(f'{classifier.predict(x[0], x[1])} == {y}?')\n",
    "    # print(classifier.predict(x[0], x[1]) == y)\n",
    "    # classifier.verbosePrediction(x[0], x[1], y)\n",
    "    # classifier.verbosePrediction(x, y)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "701b9b44",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Task 3: Compute the margin\n",
    "\n",
    "Recall, the **score** of a training example is positive if the angle between the feature vector of the training example and the weight vector is acute. And the score is negative if the angle is obtuse.\n",
    "$$\n",
    "\\mathbf{w} \\cdot \\mathbf{x} = \\Vert \\mathbf{w} \\Vert \\cdot \\Vert \\mathbf{x} \\Vert \\cdot \\cos(\\omega)\n",
    "$$\n",
    "\n",
    "All points in feature space with a score = 0, constitute the decision boundary of the classifier.\n",
    "\n",
    "Now, the **margin** takes the score and the target label and quantifies the correctness of a prediction:\n",
    "$$\n",
    "\\text{margin}(\\mathbf{x}, y, \\mathbf{w}) = (w_1 \\cdot x_1 + w_2 \\cdot x_2) \\cdot y\n",
    "$$\n",
    "\n",
    "The margin is positive if the score and the target label agree (regardless of which class it is). It is negative, if both quantities disagree.\n",
    "\n",
    "Add a method `getMargin` to your implementation and, additionally, print the margin for each training example in the `predictVerbose` method.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "830159d2",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "193cdef6",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Task 4: Compute the Hinge-Loss of the classifier\n",
    "\n",
    "Based on the margin, we can now calculate the Hinge-Loss of the classifier.\n",
    "\n",
    "Like any other loss, the Hinge Loss is large if the classifier's prediction does **not** agree with the target label.\n",
    "This is the exact opposite of what the margin tells us.\n",
    "\n",
    "Thus, we can just use the negative margin to indicate disagreement between prediction and target label.\n",
    "\n",
    "$$\n",
    "\\text{Loss}_{hinge}(x, y, \\mathbf{w}) = \\max \\{ \\text{gap} - \\underbrace{\\underbrace{(\\mathbf{w} \\cdot \\mathbf{x})}_{\\text{score}}y}_{\\text{margin}}, 0 \\}\n",
    "$$\n",
    "\n",
    "Intuitively, what is the role of $\\text{gap}$?\n",
    "- What if $gap = 0$?\n",
    "- What if $gap > 0$?\n",
    "\n",
    "Hint: Think about the closeness of training points to the decision boundary (Or rather the closeness of the decision boundary to the training points, since it's up to us to adjust the decision boundary based on the loss).\n",
    "![](linear-classifier-decision-boundary.png)\n",
    "\n",
    "Implement a method `getHingeLoss` that expects parameters $x_1$, $x_2$ and, additionally, a keyword parameter `gap` with a default value of $1.0$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8b643bc",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Task 5: Implement the weight update\n",
    "\n",
    "Based on the hinge loss, we can construct a weight update rule and repeatedly update the weights.\n",
    "\n",
    "The goal of repeated weight updates is to adjust the decision boundary of the classifier, such that it classifies the training examples correctly.\n",
    "\n",
    "For each training example, we calculate the weight change $\\Delta \\mathbf{w}$ and then update the weights using $\\Delta \\mathbf{w}$. The update rule is given as:\n",
    "\n",
    "$$\n",
    "\\mathbf{w} = \\mathbf{w} - 0.1 \\cdot \\Delta \\mathbf{w}\n",
    "$$\n",
    "\n",
    "where the weight change is given by:\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\Delta \\mathbf{w}  = \\begin{cases}\n",
    "    - [x_1 \\cdot y, x_2 \\cdot y], & \\qquad \\text{if} \\quad \\text{Loss}_{hinge}(\\mathbf{x}, y, \\mathbf{w}) > 0\\\\\n",
    "    0, & \\qquad \\text{otherwise}.\n",
    "  \\end{cases}\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "Note: We obtain the weight change by calculating the partial derivatives of the loss with respect to each weight. Example for $w_1$:\n",
    "$$\n",
    "\\begin{align}\n",
    "\\frac{\\partial \\text{Loss}_{hinge}(\\mathbf{x}, y, \\mathbf{w})}{\\partial w_1} &= \\frac{\\partial}{\\partial w_1} (\\text{gap} - (w_1 \\cdot x_1 + w_2 \\cdot x_2) \\cdot y)\\\\\n",
    "   &= \\frac{\\partial}{\\partial w_1} \\text{gap} - \\frac{\\partial}{\\partial w_1} (w_1 \\cdot x_1 + w_2 \\cdot x_2) \\cdot y)\\\\\n",
    "   &= 0 - \\frac{\\partial}{\\partial w_1} (w_1 \\cdot x_1 + w_2 \\cdot x_2) \\cdot \\frac{\\partial}{\\partial (w_1 \\cdot x_1 + w_2 \\cdot x_2)} (w_1 \\cdot x_1 + w_2 \\cdot x_2) \\cdot y \\\\\n",
    "   &= 0 - \\frac{\\partial}{\\partial w_1} (w_1 \\cdot x_1 \\cdot + w_2 \\cdot x_2)  \\cdot y\\\\\n",
    "   &= 0 - \\frac{\\partial}{\\partial w_1} (w_1 \\cdot x_1 \\cdot y)  \\cdot y\\\\\n",
    "   &= - x_1 \\cdot y\\\\\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "The derivates for $w_2$ are computed analogously.\n",
    "The weight change $\\Delta \\mathbf{w}$ is just the vector of these partial derivatives.\n",
    "\n",
    "#### Implement the update rule in a method `train`. This method expects $x_1$, $x_2$ and $y$ as parameters. You may also include an optional `eta` parameter for the learning rate, which is currently set to a fixed 0.1."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dbac805",
   "metadata": {},
   "source": [
    "### Task 6: Implement Stochastic Gradient Descent in the method `train_batch`.\n",
    "Computing updates on each training example is computationally expensive and does not yield a good sample of our ground truth. \n",
    "Instead, we should compute the updates on multiple training examples as their average.\n",
    "This also greatly reduces the variance of the updates and should lead to a more stable convergence.\n",
    "All you need to do is compute the average of the weight changes for each training example and update the weights accordingly."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "602bb4a0",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Congratulations! You have implemented the core machinery of machine learning algorithms."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e9f4253",
   "metadata": {},
   "source": [
    "# Task 7: Apply the classifier to a larger synthetic dataset and visualize the dataset as a 2d plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba2212b8",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import make_blobs\n",
    "\n",
    "X, y = make_blobs(n_samples=100, centers=2, cluster_std=1.5, random_state=0)\n",
    "\n",
    "plt.scatter(X[:, 0], X[:, 1], c=y)\n",
    "plt.title(\"Three normally-distributed clusters\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ca89a45",
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = BinaryClassifier(w1, w2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7991a177",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO use 10 samples and run a couple of times. see what happens to the loss and how long it would take to converge\n",
    "start = 0\n",
    "for epoch in range(1): # run for 1 epoch. epoch describes one full pass through the data\n",
    "for stop in range(0, len(X), 10): # a batch size of 10, used for Stochastic Gradient Descent, one update to the weights per batch\n",
    "    classifier.train_batch(X[start:stop], y[start:stop])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c77fbb11",
   "metadata": {},
   "source": [
    "## Task 8: Plot how the loss changes with each batch. for this you have to keep track of the loss in the train_batch method or in the loop where you call it\n",
    "You can use `plt.plot` to plot the loss over time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94f34a0b",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "studies",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}